Dataset:
  train: Dataset/train
  valid: Dataset/valid
  test: Dataset/test

  # Test
  use_onnx_model: False
  save_flag: True
  save_path: result
  show_flag: False
  decode_number: -1 # decode할 개수
  nms_thresh: 0.5 #0.45
  nms_topk: 1000 # 전체 다하면(-1) 너무 오래걸림.
  except_class_thresh: 0.01 #0.01
  plot_class_thresh: 0.5 # 그릴때 score_thresh 보다 큰 것들만 그린다.
  test_graph_path: test_Graph

model:
  training: True
  load_name: 400_600_ADAM_PVGG16_512 # training = False
  save_period: 200
  load_period: 200
  input_size: [400, 600] # height, width
  base: VGG16_512 # or VGG16_300
  pretrained_base: True
  pretrained_path: modelparam
  graphviz: False

hyperparameters:

  # model 관련
  image_mean: [0.485, 0.456, 0.406] # R G B
  image_std:  [0.229, 0.224, 0.225] # R G B
  box_sizes300: [21, 45, 101.25, 157.5, 213.75, 270, 326.25]
  box_ratios300: "[[1, 2, 0.5]] + [[1, 2, 0.5, 3, 1.0 / 3]] * 3 + [[1, 2, 0.5]] * 2"
  box_sizes512: [21, 51.2, 133.12, 215.04, 296.96, 378.88, 460.8, 542.72]
  box_ratios512: "[[1, 2, 0.5]] + [[1, 2, 0.5, 3, 1.0 / 3]] * 4 + [[1, 2, 0.5]] * 2"
  anchor_box_clip: True

  # 학습 관련
  epoch: 200
  batch_size: 16
  multiscale: True
  factor_scale: [8, 5] # input_size(height, width)를 8으로 나눈 값에, 6,7,8,9,10 5개를 곱한 값을 width height로 사용
  data_augmentation: True
  num_workers: 4 # the number of multiprocessing workers to use for data preprocessing.
  optimizer: ADAM # ADAM, RMSPROP, SGD
  classHardNegativeMining: True
  boxHardNegativeMining: False
  learning_rate: 0.0001
  decay_lr: 0.999
  decay_step: 100 # 몇 epoch이 지난후 decay_lr을 적용할지
  # AMP -> float32로 weight가 저장되고, 계산은 float16으로 함
  AMP: True # https://mxnet.apache.org/api/python/docs/tutorials/performance/backend/amp.html#
context:
  using_cuda: True
validation:
  eval_period: 200
  tensorboard: True
  valid_graph_path: valid_Graph # mAP 그리기
mlflow:
  using_mlflow: True
  run_name: Animal




